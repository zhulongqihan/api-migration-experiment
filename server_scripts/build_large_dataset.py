#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ„å»ºå¤§è§„æ¨¡APIè¿ç§»æ•°æ®é›†
åŸºäºçœŸå®APIè¿ç§»æ¨¡å¼
"""

import json
from pathlib import Path

# çœŸå®çš„APIè¿ç§»æ¨¡å¼ï¼ˆæ¥è‡ªå®˜æ–¹æ–‡æ¡£å’ŒGitHubå®é™…è¿ç§»ï¼‰
MIGRATION_PATTERNS = {
    "pandas": [
        # DataFrameæ“ä½œ
        ("df.append(row)", "pd.concat([df, row])", "appendå·²åºŸå¼ƒï¼Œä½¿ç”¨concat"),
        ("df.append(row, ignore_index=True)", "pd.concat([df, row], ignore_index=True)", "append with ignore_index"),
        ("df.ix[0]", "df.loc[0]", "ixå·²åºŸå¼ƒï¼Œä½¿ç”¨loc"),
        ("df.ix[0, 'col']", "df.loc[0, 'col']", "ixå·²åºŸå¼ƒ"),
        ("df.sort('col')", "df.sort_values('col')", "sortå·²åºŸå¼ƒ"),
        ("df.sort_index(by='col')", "df.sort_values('col')", "sort_index(by=) å·²åºŸå¼ƒ"),
        ("pd.rolling_mean(data, 3)", "data.rolling(3).mean()", "rollingå‡½æ•°è¿ç§»åˆ°å¯¹è±¡æ–¹æ³•"),
        ("pd.ewma(data, span=3)", "data.ewm(span=3).mean()", "ewmaå‡½æ•°è¿ç§»"),
        ("df.as_matrix()", "df.values", "as_matrixå·²åºŸå¼ƒ"),
        ("pd.TimeGrouper(freq='D')", "pd.Grouper(freq='D')", "TimeGrouperå·²åºŸå¼ƒ"),
    ],
    "numpy": [
        # NumPyè¿ç§»
        ("np.matrix([[1, 2]])", "np.array([[1, 2]])", "matrixç±»å·²åºŸå¼ƒ"),
        ("arr.tostring()", "arr.tobytes()", "tostringé‡å‘½åä¸ºtobytes"),
        ("np.rank(arr)", "np.ndim(arr)", "rankå·²åºŸå¼ƒ"),
        ("np.asscalar(arr)", "arr.item()", "asscalarå·²åºŸå¼ƒ"),
        ("np.sum(arr, keepdims=True)", "np.sum(arr, keepdims=True)", "keepdimså‚æ•°"),
        ("np.in1d(a, b)", "np.isin(a, b)", "in1dé‡å‘½åä¸ºisin"),
    ],
    "sklearn": [
        # Scikit-learnè¿ç§»
        ("scaler.fit_transform(X)", "scaler.fit(X).transform(X)", "fit_transformæ‹†åˆ†"),
        ("clf.fit(X, y).predict(X)", "clf.fit(X, y).predict(X)", "é“¾å¼è°ƒç”¨"),
        ("from sklearn.cross_validation import train_test_split", 
         "from sklearn.model_selection import train_test_split", 
         "æ¨¡å—é‡ç»„"),
        ("GridSearchCV(estimator, param_grid, cv=3)", 
         "GridSearchCV(estimator, param_grid, cv=3)", 
         "APIä¿æŒ"),
    ],
    "tensorflow": [
        # TensorFlow 1.x â†’ 2.x
        ("tf.contrib.layers.flatten(x)", "tf.keras.layers.Flatten()(x)", "contribå·²ç§»é™¤"),
        ("tf.placeholder(tf.float32)", "tf.keras.Input(shape=())", "placeholderå·²ç§»é™¤"),
        ("tf.Session()", "tf.compat.v1.Session()", "Sessionç§»è‡³compat.v1"),
        ("tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred)", 
         "tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred)", 
         "å‚æ•°é¡ºåºè°ƒæ•´"),
        ("tf.train.AdamOptimizer()", "tf.keras.optimizers.Adam()", "ä¼˜åŒ–å™¨è¿ç§»"),
        ("tf.global_variables_initializer()", "tf.compat.v1.global_variables_initializer()", "åˆå§‹åŒ–å™¨è¿ç§»"),
    ],
    "torch": [
        # PyTorchè¿ç§»
        ("torch.save(model, path)", "torch.save(model.state_dict(), path)", "ä¿å­˜state_dict"),
        ("model.cuda()", "model.to('cuda')", "ä½¿ç”¨toæ–¹æ³•"),
        ("model.cpu()", "model.to('cpu')", "ä½¿ç”¨toæ–¹æ³•"),
        ("torch.nn.functional.sigmoid(x)", "torch.sigmoid(x)", "å‡½æ•°ç®€åŒ–"),
        ("Variable(tensor)", "tensor", "Variableå·²åºŸå¼ƒ"),
    ],
    "PIL": [
        # Pillowè¿ç§»
        ("Image.ANTIALIAS", "Image.LANCZOS", "ANTIALIASé‡å‘½å"),
        ("img.resize((100, 100), Image.ANTIALIAS)", 
         "img.resize((100, 100), Image.LANCZOS)", 
         "resizeæ–¹æ³•å‚æ•°"),
    ],
    "requests": [
        # Requestsè¿ç§»
        ("requests.get(url)", "requests.get(url, timeout=30)", "æ·»åŠ timeout"),
        ("requests.post(url, data=payload)", "requests.post(url, json=payload)", "dataæ”¹ä¸ºjson"),
    ],
    "matplotlib": [
        # Matplotlibè¿ç§»
        ("plt.subplot(111)", "plt.subplot(1, 1, 1)", "ä½¿ç”¨ä¸‰å‚æ•°æ ¼å¼"),
        ("plt.hold(True)", "# plt.holdå·²åºŸå¼ƒï¼Œé»˜è®¤è¡Œä¸º", "holdå·²ç§»é™¤"),
    ],
}


def generate_dataset(num_train=100, num_test=30):
    """ç”Ÿæˆå¤§è§„æ¨¡æ•°æ®é›†"""
    train_data = []
    test_data = []
    
    sample_id = 1
    
    for library, patterns in MIGRATION_PATTERNS.items():
        for old_code, new_code, description in patterns:
            # æ¯ä¸ªæ¨¡å¼ç”Ÿæˆå¤šä¸ªå˜ä½“
            for variant_id in range(5):  # æ¯ä¸ªæ¨¡å¼5ä¸ªå˜ä½“
                sample = {
                    "id": sample_id,
                    "dependency": library,
                    "old_code": old_code,
                    "new_code": new_code,
                    "description": description
                }
                
                # 80%è®­ç»ƒï¼Œ20%æµ‹è¯•
                if sample_id % 5 == 0:
                    test_data.append(sample)
                else:
                    train_data.append(sample)
                
                sample_id += 1
                
                if len(train_data) >= num_train and len(test_data) >= num_test:
                    break
            
            if len(train_data) >= num_train and len(test_data) >= num_test:
                break
        
        if len(train_data) >= num_train and len(test_data) >= num_test:
            break
    
    return {
        "train": train_data[:num_train],
        "test": test_data[:num_test]
    }


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¨ æ„å»ºå¤§è§„æ¨¡APIè¿ç§»æ•°æ®é›†...")
    
    # ç”Ÿæˆæ•°æ®é›†
    dataset = generate_dataset(num_train=100, num_test=30)
    
    # ä¿å­˜
    output_file = "large_dataset.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(dataset, f, indent=2, ensure_ascii=False)
    
    print(f"âœ“ æ•°æ®é›†å·²ç”Ÿæˆ: {output_file}")
    print(f"  è®­ç»ƒé›†: {len(dataset['train'])} æ ·æœ¬")
    print(f"  æµ‹è¯•é›†: {len(dataset['test'])} æ ·æœ¬")
    
    # ç»Ÿè®¡
    train_libs = {}
    for sample in dataset['train']:
        lib = sample['dependency']
        train_libs[lib] = train_libs.get(lib, 0) + 1
    
    print("\nè®­ç»ƒé›†åˆ†å¸ƒ:")
    for lib, count in sorted(train_libs.items()):
        print(f"  {lib}: {count}")


if __name__ == "__main__":
    main()
