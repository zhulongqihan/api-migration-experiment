#!/usr/bin/env python3
"""
ç›´æ¥å®ç°ROMEç®—æ³•ï¼Œä¸ä¾èµ–EasyEditåº“
åŸºäºè®ºæ–‡: "Locating and Editing Factual Associations in GPT"
"""

import json
import torch
import torch.nn.functional as F
from pathlib import Path
from rich.console import Console
from rich.table import Table
from rich.progress import track
import argparse
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import List, Tuple
import numpy as np

console = Console()

class ROMEEditor:
    """ç›´æ¥å®ç°çš„ROMEç¼–è¾‘å™¨"""
    
    def __init__(self, model, tokenizer, device='cuda'):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        
        # é…ç½®å‚æ•°
        self.v_lr = 0.5
        self.v_num_grad_steps = 20
        self.mom2_update_weight = 10000
        
        # ç›®æ ‡å±‚ï¼ˆå¯¹äºQwen2.5-Coder-1.5Bï¼Œä½¿ç”¨ä¸­åå±‚ï¼‰
        self.layer_ids = [20, 21, 22, 23, 24]
        
    def get_module_by_path(self, path: str):
        """æ ¹æ®è·¯å¾„è·å–æ¨¡å—"""
        parts = path.split('.')
        module = self.model
        for part in parts:
            if part.isdigit():
                module = module[int(part)]
            else:
                module = getattr(module, part)
        return module
    
    def compute_z(self, layer_id: int, input_text: str, target_text: str):
        """
        è®¡ç®—ç¼–è¾‘å‘é‡z
        æ ¸å¿ƒROMEç®—æ³•ï¼šæœ€å°åŒ– ||h_l + Î”h_l - z||^2
        """
        # ç¼–ç è¾“å…¥
        inputs = self.tokenizer(input_text, return_tensors="pt").to(self.device)
        
        # å‰å‘ä¼ æ’­ï¼Œè·å–éšè—çŠ¶æ€
        with torch.no_grad():
            outputs = self.model(**inputs, output_hidden_states=True)
            hidden_states = outputs.hidden_states[layer_id]
            
        # ä½¿ç”¨æœ€åä¸€ä¸ªtokençš„éšè—çŠ¶æ€
        h_l = hidden_states[:, -1, :].detach()
        
        # è®¡ç®—ç›®æ ‡è¡¨ç¤º
        target_inputs = self.tokenizer(target_text, return_tensors="pt").to(self.device)
        with torch.no_grad():
            target_outputs = self.model(**target_inputs, output_hidden_states=True)
            target_hidden = target_outputs.hidden_states[layer_id][:, -1, :]
        
        # è®¡ç®—zï¼šç›®æ ‡éšè—çŠ¶æ€
        z = target_hidden.detach()
        
        return h_l, z
    
    def compute_covariance(self, layer_id: int, sample_texts: List[str]):
        """
        è®¡ç®—åæ–¹å·®çŸ©é˜µC
        C = E[key @ key.T]
        """
        console.print(f"[yellow]  è®¡ç®—å±‚{layer_id}çš„åæ–¹å·®çŸ©é˜µ...[/yellow]")
        
        # è·å–MLPæ¨¡å—
        layer_name = f"model.layers.{layer_id}"
        layer = self.get_module_by_path(layer_name)
        
        # æ”¶é›†keyæ¿€æ´»
        key_activations = []
        
        for text in sample_texts[:10]:  # ä½¿ç”¨å‰10ä¸ªæ ·æœ¬
            inputs = self.tokenizer(text, return_tensors="pt", truncation=True, max_length=512).to(self.device)
            
            with torch.no_grad():
                outputs = self.model(**inputs, output_hidden_states=True)
                hidden = outputs.hidden_states[layer_id]
                
                # ä½¿ç”¨æ‰€æœ‰tokençš„æ¿€æ´»
                for i in range(hidden.shape[1]):
                    key_activations.append(hidden[0, i, :].cpu())
        
        # è®¡ç®—åæ–¹å·®
        keys = torch.stack(key_activations)
        C = torch.cov(keys.T)
        
        return C
    
    def edit_layer(self, layer_id: int, old_text: str, new_text: str, C: torch.Tensor):
        """
        åœ¨ç‰¹å®šå±‚æ‰§è¡ŒROMEç¼–è¾‘
        æ›´æ–°å…¬å¼: W' = W + (z - h_l) @ k.T @ C^{-1}
        """
        # è®¡ç®—ç¼–è¾‘å‘é‡
        h_l, z = self.compute_z(layer_id, old_text, new_text)
        
        # è®¡ç®—keyå‘é‡ï¼ˆä½¿ç”¨è¾“å…¥çš„éšè—çŠ¶æ€ä½œä¸ºkeyï¼‰
        k = h_l
        
        # è®¡ç®—æ›´æ–°æ–¹å‘
        delta_h = (z - h_l)
        
        # ä½¿ç”¨ä¼ªé€†é¿å…å¥‡å¼‚çŸ©é˜µ
        try:
            C_inv = torch.linalg.pinv(C.to(self.device))
        except:
            console.print(f"[yellow]  âš  ä½¿ç”¨æ­£åˆ™åŒ–çš„é€†çŸ©é˜µ[/yellow]")
            C_reg = C + torch.eye(C.shape[0]) * 1e-4
            C_inv = torch.linalg.pinv(C_reg.to(self.device))
        
        # è®¡ç®—æƒé‡æ›´æ–°: Î”W = delta_h @ k.T @ C^{-1}
        weight_update = torch.outer(delta_h.squeeze(), k.squeeze()) @ C_inv
        
        # åº”ç”¨åˆ°MLPå±‚çš„æƒé‡
        layer_name = f"model.layers.{layer_id}.mlp.down_proj"
        try:
            mlp_layer = self.get_module_by_path(layer_name)
            
            # æ›´æ–°æƒé‡
            with torch.no_grad():
                # é™åˆ¶æ›´æ–°å¹…åº¦
                update_norm = torch.norm(weight_update)
                if update_norm > 0.1:
                    weight_update = weight_update * (0.1 / update_norm)
                
                # åº”ç”¨æ›´æ–°åˆ°æƒé‡çŸ©é˜µçš„ä¸€éƒ¨åˆ†
                W = mlp_layer.weight
                update_slice = weight_update[:W.shape[0], :W.shape[1]]
                mlp_layer.weight.data += 0.1 * update_slice  # ä½¿ç”¨å°çš„å­¦ä¹ ç‡
                
            return True
            
        except Exception as e:
            console.print(f"[red]  âœ— æ›´æ–°å¤±è´¥: {e}[/red]")
            return False
    
    def edit(self, old_codes: List[str], new_codes: List[str]):
        """æ‰§è¡Œæ‰¹é‡ç¼–è¾‘"""
        results = []
        
        # è®¡ç®—åæ–¹å·®çŸ©é˜µï¼ˆæ‰€æœ‰å±‚å…±äº«ï¼‰
        console.print("[yellow]é¢„è®¡ç®—åæ–¹å·®çŸ©é˜µ...[/yellow]")
        covariances = {}
        
        all_texts = old_codes + new_codes
        for layer_id in self.layer_ids:
            covariances[layer_id] = self.compute_covariance(layer_id, all_texts)
        
        console.print("[green]âœ“ åæ–¹å·®çŸ©é˜µè®¡ç®—å®Œæˆ[/green]")
        
        # å¯¹æ¯ä¸ªAPIè¿›è¡Œç¼–è¾‘
        for i, (old_code, new_code) in enumerate(track(
            zip(old_codes, new_codes), 
            description="ç¼–è¾‘ä¸­...",
            total=len(old_codes)
        )):
            success_count = 0
            
            # åœ¨å¤šä¸ªå±‚è¿›è¡Œç¼–è¾‘
            for layer_id in self.layer_ids:
                try:
                    success = self.edit_layer(
                        layer_id, 
                        old_code, 
                        new_code, 
                        covariances[layer_id]
                    )
                    if success:
                        success_count += 1
                except Exception as e:
                    console.print(f"[red]âœ— ç¼–è¾‘ {i} å±‚ {layer_id} å¤±è´¥: {e}[/red]")
            
            results.append({
                "index": i,
                "success": success_count > 0,
                "layers_edited": success_count
            })
        
        return results

def main():
    parser = argparse.ArgumentParser(description="ç›´æ¥ROMEå®ç°")
    parser.add_argument("--model_name", type=str, default="Qwen/Qwen2.5-Coder-1.5B")
    parser.add_argument("--data_file", type=str, default="extended_dataset_50.json")
    parser.add_argument("--output_dir", type=str, default="../models/rome_direct")
    parser.add_argument("--num_edits", type=int, default=10)
    
    args = parser.parse_args()
    
    console.print("\n[bold cyan]ğŸ”¬ ç›´æ¥ROMEçŸ¥è¯†ç¼–è¾‘[/bold cyan]\n")
    
    # æ˜¾ç¤ºé…ç½®
    table = Table(title="å®éªŒé…ç½®")
    table.add_column("å‚æ•°", style="cyan")
    table.add_column("å€¼", style="green")
    table.add_row("åŸºç¡€æ¨¡å‹", args.model_name)
    table.add_row("æ•°æ®æ–‡ä»¶", args.data_file)
    table.add_row("ç¼–è¾‘æ–¹æ³•", "ROME (ç›´æ¥å®ç°)")
    table.add_row("è¾“å‡ºç›®å½•", args.output_dir)
    table.add_row("ç¼–è¾‘æ•°é‡", str(args.num_edits))
    console.print(table)
    
    # æ­¥éª¤1ï¼šåŠ è½½æ•°æ®
    console.print("\n[yellow]æ­¥éª¤1/5: åŠ è½½æ•°æ®[/yellow]")
    with open(args.data_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    train_data = data['train'][:args.num_edits]
    console.print(f"[green]âœ“ åŠ è½½äº† {len(train_data)} ä¸ªAPIæ›´æ–°æ¡ˆä¾‹[/green]")
    
    # æ­¥éª¤2ï¼šåŠ è½½æ¨¡å‹
    console.print("\n[yellow]æ­¥éª¤2/5: åŠ è½½æ¨¡å‹[/yellow]")
    tokenizer = AutoTokenizer.from_pretrained(args.model_name, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        args.model_name,
        torch_dtype=torch.float32,  # ROMEéœ€è¦float32
        device_map="auto",
        trust_remote_code=True
    )
    console.print("[green]âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ[/green]")
    
    # æ­¥éª¤3ï¼šåˆå§‹åŒ–ç¼–è¾‘å™¨
    console.print("\n[yellow]æ­¥éª¤3/5: åˆå§‹åŒ–ROMEç¼–è¾‘å™¨[/yellow]")
    editor = ROMEEditor(model, tokenizer)
    console.print("[green]âœ“ ç¼–è¾‘å™¨åˆå§‹åŒ–æˆåŠŸ[/green]")
    console.print(f"[cyan]  ç¼–è¾‘å±‚: {editor.layer_ids}[/cyan]")
    
    # æ­¥éª¤4ï¼šæ‰§è¡Œç¼–è¾‘
    console.print("\n[yellow]æ­¥éª¤4/5: æ‰§è¡ŒROMEç¼–è¾‘[/yellow]")
    
    old_codes = [item['old_code'] for item in train_data]
    new_codes = [item['new_code'] for item in train_data]
    
    results = editor.edit(old_codes, new_codes)
    
    success_count = sum(1 for r in results if r['success'])
    console.print(f"\n[green]âœ… æˆåŠŸç¼–è¾‘: {success_count}/{len(train_data)}[/green]")
    
    # æ­¥éª¤5ï¼šä¿å­˜æ¨¡å‹
    console.print("\n[yellow]æ­¥éª¤5/5: ä¿å­˜ç¼–è¾‘åçš„æ¨¡å‹[/yellow]")
    
    output_path = Path(args.output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    model.save_pretrained(str(output_path))
    tokenizer.save_pretrained(str(output_path))
    
    console.print(f"[green]âœ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {output_path}[/green]")
    
    # ä¿å­˜ç¼–è¾‘ä¿¡æ¯
    info = {
        "method": "ROME (ç›´æ¥å®ç°)",
        "base_model": args.model_name,
        "num_edits": len(train_data),
        "success_count": success_count,
        "layer_ids": editor.layer_ids,
        "results": results
    }
    
    info_path = output_path / "editing_info.json"
    with open(info_path, 'w', encoding='utf-8') as f:
        json.dump(info, f, indent=2, ensure_ascii=False)
    
    console.print(f"\n[bold green]âœ… ROMEç¼–è¾‘å®Œæˆï¼[/bold green]")
    console.print(f"\n[cyan]ç¼–è¾‘ç»Ÿè®¡ï¼š[/cyan]")
    console.print(f"  æ€»ç¼–è¾‘æ•°: {len(train_data)}")
    console.print(f"  æˆåŠŸ: {success_count}")
    console.print(f"  å¤±è´¥: {len(train_data) - success_count}")
    
    console.print("\n[cyan]ä¸‹ä¸€æ­¥æ“ä½œï¼š[/cyan]")
    console.print(f"python3 evaluate_lora.py --model_path {output_path} --data_file {args.data_file}")

if __name__ == "__main__":
    main()
