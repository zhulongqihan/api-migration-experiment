#!/usr/bin/env python3
"""
ä½¿ç”¨ROME (Rank-One Model Editing) è¿›è¡ŒAPIçŸ¥è¯†ç¼–è¾‘
æ›´ç®€å•ã€æ›´ç›´æ¥çš„å®ç°
"""

import json
import torch
import numpy as np
from pathlib import Path
from rich.console import Console
from rich.table import Table
from rich.progress import track
import argparse

console = Console()

class SimpleROME:
    """ç®€åŒ–çš„ROMEå®ç°ï¼Œé€‚ç”¨äºä»£ç APIæ›´æ–°"""
    
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.device = next(model.parameters()).device
    
    def locate_api_knowledge(self, old_api, new_api):
        """å®šä½éœ€è¦ç¼–è¾‘çš„æ¨¡å‹å±‚"""
        # ç®€åŒ–å®ç°ï¼šæ‰¾åˆ°æ·±å±‚transformerå±‚ï¼ˆé€šå¸¸çŸ¥è¯†å­˜å‚¨åœ¨è¿™é‡Œï¼‰
        # å¯¹äºQwen2.5-Coder-1.5Bï¼Œä½¿ç”¨ç¬¬20-25å±‚
        target_layers = list(range(20, 26))
        return target_layers
    
    def compute_edit_vector(self, old_code, new_code):
        """è®¡ç®—ç¼–è¾‘å‘é‡"""
        # ç¼–ç æ—§ä»£ç å’Œæ–°ä»£ç 
        old_inputs = self.tokenizer(old_code, return_tensors="pt").to(self.device)
        new_inputs = self.tokenizer(new_code, return_tensors="pt").to(self.device)
        
        with torch.no_grad():
            old_outputs = self.model(**old_inputs, output_hidden_states=True)
            new_outputs = self.model(**new_inputs, output_hidden_states=True)
        
        # è®¡ç®—æ·±å±‚hidden statesçš„å·®å¼‚
        layer_idx = 24  # ä½¿ç”¨å€’æ•°ç¬¬5å±‚
        old_hidden = old_outputs.hidden_states[layer_idx].mean(dim=1)
        new_hidden = new_outputs.hidden_states[layer_idx].mean(dim=1)
        
        edit_vector = new_hidden - old_hidden
        return edit_vector, layer_idx
    
    def apply_edit(self, edit_vector, layer_idx, strength=0.5):
        """åº”ç”¨ç¼–è¾‘åˆ°æ¨¡å‹"""
        # è·å–ç›®æ ‡å±‚
        target_layer = self.model.model.layers[layer_idx]
        
        # å¯¹MLPå±‚è¿›è¡Œç§©1æ›´æ–°
        mlp = target_layer.mlp
        
        # æ›´æ–°æƒé‡ï¼ˆç®€åŒ–ç‰ˆï¼‰
        with torch.no_grad():
            # åªæ›´æ–°è¾“å‡ºæŠ•å½±
            if hasattr(mlp, 'down_proj'):
                weight = mlp.down_proj.weight
                # ç§©1æ›´æ–°: W_new = W + strength * v * v^T
                update = strength * edit_vector.T @ edit_vector
                # é™åˆ¶æ›´æ–°å¹…åº¦
                update = torch.clamp(update, -0.01, 0.01)
                mlp.down_proj.weight.add_(update[:weight.shape[0], :weight.shape[1]])
        
        return True

def main():
    parser = argparse.ArgumentParser(description="ä½¿ç”¨ç®€åŒ–ROMEè¿›è¡ŒAPIçŸ¥è¯†ç¼–è¾‘")
    parser.add_argument("--model_name", type=str, default="Qwen/Qwen2.5-Coder-1.5B")
    parser.add_argument("--data_file", type=str, default="extended_dataset_50.json")
    parser.add_argument("--output_dir", type=str, default="../models/rome_edited")
    parser.add_argument("--num_edits", type=int, default=10, help="ç¼–è¾‘çš„APIæ•°é‡")
    parser.add_argument("--strength", type=float, default=0.5, help="ç¼–è¾‘å¼ºåº¦")
    
    args = parser.parse_args()
    
    console.print("\n[bold cyan]ğŸ”¬ ROMEçŸ¥è¯†ç¼–è¾‘å®éªŒ[/bold cyan]\n")
    
    # æ˜¾ç¤ºé…ç½®
    table = Table(title="å®éªŒé…ç½®")
    table.add_column("å‚æ•°", style="cyan")
    table.add_column("å€¼", style="green")
    table.add_row("åŸºç¡€æ¨¡å‹", args.model_name)
    table.add_row("æ•°æ®æ–‡ä»¶", args.data_file)
    table.add_row("ç¼–è¾‘æ–¹æ³•", "ROME (ç®€åŒ–ç‰ˆ)")
    table.add_row("è¾“å‡ºç›®å½•", args.output_dir)
    table.add_row("ç¼–è¾‘æ•°é‡", str(args.num_edits))
    table.add_row("ç¼–è¾‘å¼ºåº¦", str(args.strength))
    console.print(table)
    
    # æ­¥éª¤1ï¼šåŠ è½½æ•°æ®
    console.print("\n[yellow]æ­¥éª¤1/5: åŠ è½½æ•°æ®[/yellow]")
    with open(args.data_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    train_data = data['train'][:args.num_edits]
    console.print(f"[green]âœ“ åŠ è½½äº† {len(train_data)} ä¸ªAPIæ›´æ–°æ¡ˆä¾‹[/green]")
    
    # æ­¥éª¤2ï¼šåŠ è½½æ¨¡å‹
    console.print("\n[yellow]æ­¥éª¤2/5: åŠ è½½æ¨¡å‹[/yellow]")
    from transformers import AutoTokenizer, AutoModelForCausalLM
    
    tokenizer = AutoTokenizer.from_pretrained(args.model_name, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        args.model_name,
        torch_dtype=torch.float32,  # ROMEéœ€è¦float32
        device_map="auto",
        trust_remote_code=True
    )
    console.print("[green]âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ[/green]")
    
    # æ­¥éª¤3ï¼šåˆå§‹åŒ–ROMEç¼–è¾‘å™¨
    console.print("\n[yellow]æ­¥éª¤3/5: åˆå§‹åŒ–ROMEç¼–è¾‘å™¨[/yellow]")
    editor = SimpleROME(model, tokenizer)
    console.print("[green]âœ“ ç¼–è¾‘å™¨åˆå§‹åŒ–æˆåŠŸ[/green]")
    
    # æ­¥éª¤4ï¼šæ‰§è¡Œç¼–è¾‘
    console.print("\n[yellow]æ­¥éª¤4/5: æ‰§è¡ŒçŸ¥è¯†ç¼–è¾‘[/yellow]")
    
    edit_results = []
    for i, item in enumerate(track(train_data, description="ç¼–è¾‘ä¸­...")):
        try:
            # è®¡ç®—ç¼–è¾‘å‘é‡
            edit_vector, layer_idx = editor.compute_edit_vector(
                item['old_code'], 
                item['new_code']
            )
            
            # åº”ç”¨ç¼–è¾‘
            success = editor.apply_edit(edit_vector, layer_idx, args.strength)
            
            edit_results.append({
                "index": i,
                "library": item.get('library', 'unknown'),
                "layer": layer_idx,
                "success": success
            })
            
        except Exception as e:
            console.print(f"[red]âœ— ç¼–è¾‘ {i} å¤±è´¥: {e}[/red]")
            edit_results.append({
                "index": i,
                "success": False,
                "error": str(e)
            })
    
    success_count = sum(1 for r in edit_results if r.get("success", False))
    console.print(f"\n[green]âœ… æˆåŠŸç¼–è¾‘: {success_count}/{len(train_data)}[/green]")
    
    # æ­¥éª¤5ï¼šä¿å­˜ç¼–è¾‘åçš„æ¨¡å‹
    console.print("\n[yellow]æ­¥éª¤5/5: ä¿å­˜æ¨¡å‹[/yellow]")
    
    output_path = Path(args.output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    model.save_pretrained(str(output_path))
    tokenizer.save_pretrained(str(output_path))
    
    console.print(f"[green]âœ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {output_path}[/green]")
    
    # ä¿å­˜ç¼–è¾‘ä¿¡æ¯
    info = {
        "method": "ROME (Simplified)",
        "base_model": args.model_name,
        "num_edits": len(train_data),
        "success_count": success_count,
        "edit_strength": args.strength,
        "edit_results": edit_results
    }
    
    info_path = output_path / "editing_info.json"
    with open(info_path, 'w', encoding='utf-8') as f:
        json.dump(info, f, indent=2, ensure_ascii=False)
    
    console.print(f"\n[bold green]âœ… çŸ¥è¯†ç¼–è¾‘å®Œæˆï¼[/bold green]")
    console.print(f"\n[cyan]ç¼–è¾‘ç»Ÿè®¡ï¼š[/cyan]")
    console.print(f"  æ€»ç¼–è¾‘æ•°: {len(train_data)}")
    console.print(f"  æˆåŠŸ: {success_count}")
    console.print(f"  å¤±è´¥: {len(train_data) - success_count}")
    
    console.print("\n[cyan]ä¸‹ä¸€æ­¥æ“ä½œï¼š[/cyan]")
    console.print(f"python3 evaluate_lora.py --model_path {output_path} --data_file {args.data_file}")

if __name__ == "__main__":
    main()
