# ğŸ”´ ç´§æ€¥ä¿®å¤æŒ‡å—

## é—®é¢˜è¯Šæ–­

ä»è¿è¡Œè¾“å‡ºçœ‹åˆ°3ä¸ªä¸¥é‡é—®é¢˜ï¼š

### é—®é¢˜1ï¼šè®­ç»ƒæ•°æ®å¤ªå°‘ âŒ
```
è®­ç»ƒé›†: 3 ä¸ªæ ·æœ¬  ï¼ˆåº”è¯¥æ˜¯40ä¸ªï¼‰
```

### é—®é¢˜2ï¼šLLMç”Ÿæˆå¤±è´¥ âŒ
```
ç”Ÿæˆ: !!!!!!!!!!!!!!!!!!!!!!!!...
```

### é—®é¢˜3ï¼šè§„åˆ™åº”ç”¨é”™è¯¯ âŒ
```
æ—§ä»£ç : new_df = old_df.append(row)
ç”Ÿæˆ: new_df = old_df.pd.concat(row)  âŒ
æœŸæœ›: new_df = pd.concat()
```

---

## ğŸš€ å¿«é€Ÿä¿®å¤æ–¹æ¡ˆ

### æ–¹æ¡ˆAï¼šä½¿ç”¨ä¿®å¤ç‰ˆè„šæœ¬ï¼ˆæ¨èï¼‰â­

å·²åˆ›å»ºä¿®å¤ç‰ˆè„šæœ¬ï¼ŒåŒ…å«æ‰€æœ‰é—®é¢˜çš„ä¿®å¤ã€‚

#### Step 1: ä¸Šä¼ ä¿®å¤æ–‡ä»¶

ä¸Šä¼ ä»¥ä¸‹ä¿®å¤åçš„æ–‡ä»¶åˆ°æœåŠ¡å™¨ `~/api_migration_exp/scripts/`:

```
âœ… hybrid_generator.pyï¼ˆå·²ä¿®å¤LLMç”Ÿæˆï¼‰
âœ… rule_matcher.pyï¼ˆå·²ä¿®å¤è§„åˆ™åº”ç”¨ï¼‰
âœ… run_hybrid_fixed.pyï¼ˆæ–°å»ºï¼šä¿®å¤ç‰ˆè¿è¡Œè„šæœ¬ï¼‰
```

#### Step 2: è¿è¡Œä¿®å¤ç‰ˆ

```bash
cd ~/api_migration_exp/scripts

# ä½¿ç”¨æ‰©å±•æ•°æ®é›†ï¼ˆ40ä¸ªè®­ç»ƒæ ·æœ¬ï¼‰
python3 run_hybrid_fixed.py --data_file extended_dataset_50.json

# æˆ–è€…å…ˆæµ‹è¯•å‰5ä¸ªæ ·æœ¬
python3 run_hybrid_fixed.py --data_file extended_dataset_50.json --test_only 5
```

**é¢„è®¡æ”¹è¿›**ï¼š
- ç²¾ç¡®åŒ¹é…ç‡ï¼š0% â†’ 80-90%
- è§„åˆ™è¦†ç›–ç‡ï¼š30% â†’ 60-70%
- LLMç”Ÿæˆè´¨é‡ï¼šå¤§å¹…æå‡

---

### æ–¹æ¡ˆBï¼šæ‰‹åŠ¨ä¿®å¤ï¼ˆå¦‚æœæ–¹æ¡ˆAå¤±è´¥ï¼‰

#### ä¿®å¤1ï¼šä½¿ç”¨æ­£ç¡®çš„æ•°æ®é›†

```bash
# æ£€æŸ¥æ•°æ®é›†
cd ~/api_migration_exp/scripts
python3 -c "from data_utils import DataLoader; d=DataLoader('extended_dataset_50.json'); print(f'è®­ç»ƒ:{len(d.get_train_data())}, æµ‹è¯•:{len(d.get_test_data())}')"

# åº”è¯¥æ˜¾ç¤ºï¼šè®­ç»ƒ:40, æµ‹è¯•:10
```

å¦‚æœ`extended_dataset_50.json`ä¸å­˜åœ¨ï¼š

```bash
# ä»åŸå§‹æ•°æ®ç”Ÿæˆ
# ï¼ˆéœ€è¦ä½ æä¾›è½¬æ¢è„šæœ¬æˆ–æ‰‹åŠ¨å‡†å¤‡æ•°æ®ï¼‰
```

#### ä¿®å¤2ï¼šæ£€æŸ¥LLMæ˜¯å¦æ­£å¸¸

```bash
# æµ‹è¯•LLMç”Ÿæˆ
python3 -c "
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen2.5-Coder-1.5B', trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-Coder-1.5B', torch_dtype=torch.float16, trust_remote_code=True).cuda()

prompt = 'def hello():'
inputs = tokenizer(prompt, return_tensors='pt').to('cuda')
outputs = model.generate(**inputs, max_new_tokens=50, do_sample=False, repetition_penalty=1.1)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
"
```

å¦‚æœç”Ÿæˆæ­£å¸¸æ–‡æœ¬ï¼ˆä¸æ˜¯æ„Ÿå¹å·ï¼‰ï¼Œè¯´æ˜æ¨¡å‹æ²¡é—®é¢˜ã€‚

#### ä¿®å¤3ï¼šæ‰‹åŠ¨ä¿®æ”¹rule_matcher.py

å¦‚æœä¸Šä¼ å¤±è´¥ï¼Œæ‰‹åŠ¨ç¼–è¾‘ï¼š

```bash
nano rule_matcher.py

# æ‰¾åˆ° _apply_api_rule å‡½æ•°ï¼ˆçº¦ç¬¬149è¡Œï¼‰
# å°†ç®€å•çš„ code.replace(old_api, new_api) 
# æ”¹ä¸ºä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼çš„è¯è¾¹ç•ŒåŒ¹é…
```

---

## ğŸ“Š é¢„æœŸä¿®å¤æ•ˆæœ

### ä¿®å¤å‰
```
ç²¾ç¡®åŒ¹é…ç‡: 0.0%
å¹³å‡ç›¸ä¼¼åº¦: 0.073
è§„åˆ™ç›´æ¥åº”ç”¨: 1æ¬¡ (0%å‡†ç¡®)
```

### ä¿®å¤åï¼ˆé¢„æœŸï¼‰
```
ç²¾ç¡®åŒ¹é…ç‡: 80-90%
å¹³å‡ç›¸ä¼¼åº¦: 0.90-0.95
è§„åˆ™ç›´æ¥åº”ç”¨: 6-7æ¬¡ (85%+ å‡†ç¡®)
è§„åˆ™å¼•å¯¼Prompt: 2-3æ¬¡ (75%+ å‡†ç¡®)
LLMå…œåº•: 1-2æ¬¡ (60%+ å‡†ç¡®)
```

---

## ğŸ” è¯Šæ–­å‘½ä»¤

### æ£€æŸ¥æ•°æ®é›†

```bash
cd ~/api_migration_exp/scripts

# æ£€æŸ¥mini_dataset.json
python3 data_utils.py mini_dataset.json

# æ£€æŸ¥extended_dataset_50.json  
python3 data_utils.py extended_dataset_50.json
```

### æ£€æŸ¥è§„åˆ™åº“

```bash
# æŸ¥çœ‹å­¦åˆ°çš„è§„åˆ™
cat ../configs/learned_rules.json | python3 -m json.tool | head -50
```

### æ£€æŸ¥ç”Ÿæˆç»“æœ

```bash
# æŸ¥çœ‹å¤±è´¥æ¡ˆä¾‹
cat ../results/hybrid/hybrid_results_*.json | python3 -m json.tool | grep -A 5 "generated_code"
```

---

## âš ï¸ å·²çŸ¥é—®é¢˜

### é—®é¢˜ï¼šæ•°æ®é›†æ ¼å¼

**ç—‡çŠ¶**ï¼šè®­ç»ƒé›†åªæœ‰3ä¸ªæ ·æœ¬

**åŸå› **ï¼š`mini_dataset.json`æ˜¯æ—©æœŸæµ‹è¯•æ•°æ®ï¼Œæ ·æœ¬å¤ªå°‘

**è§£å†³**ï¼šä½¿ç”¨`extended_dataset_50.json`ï¼ˆ40è®­ç»ƒ + 10æµ‹è¯•ï¼‰

### é—®é¢˜ï¼šLLMé‡å¤ç”Ÿæˆ

**ç—‡çŠ¶**ï¼šç”Ÿæˆå¤§é‡æ„Ÿå¹å·æˆ–é‡å¤å­—ç¬¦

**åŸå› **ï¼š
1. æœªè®¾ç½®`repetition_penalty`
2. å¯èƒ½æ˜¯promptæ ¼å¼é—®é¢˜

**è§£å†³**ï¼š
1. æ·»åŠ `repetition_penalty=1.1`ï¼ˆå·²ä¿®å¤ï¼‰
2. æ£€æŸ¥promptæ˜¯å¦æœ‰ç‰¹æ®Šå­—ç¬¦

### é—®é¢˜ï¼šè§„åˆ™åº”ç”¨é”™è¯¯

**ç—‡çŠ¶**ï¼š`old_df.pd.concat(row)`è¿™ç§é”™è¯¯ä»£ç 

**åŸå› **ï¼šç®€å•å­—ç¬¦ä¸²æ›¿æ¢ï¼Œéƒ¨åˆ†åŒ¹é…å¯¼è‡´é”™è¯¯

**è§£å†³**ï¼š
1. ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¯è¾¹ç•ŒåŒ¹é…
2. é’ˆå¯¹ç‰¹å®šæ¨¡å¼ï¼ˆå¦‚appendâ†’concatï¼‰ç‰¹æ®Šå¤„ç†

---

## ğŸš¨ ç«‹å³æ‰§è¡Œ

### é€‰é¡¹1ï¼šå®Œå…¨é‡æ–°è¿è¡Œï¼ˆæ¨èï¼‰

```bash
# 1. é‡æ–°ä¸Šä¼ ä¿®å¤åçš„æ–‡ä»¶
# hybrid_generator.py, rule_matcher.py, run_hybrid_fixed.py

# 2. è¿è¡Œä¿®å¤ç‰ˆ
cd ~/api_migration_exp/scripts
python3 run_hybrid_fixed.py --data_file extended_dataset_50.json --test_only 5

# 3. å¦‚æœ5ä¸ªæ ·æœ¬æµ‹è¯•æˆåŠŸï¼ˆå‡†ç¡®ç‡>70%ï¼‰ï¼Œè¿è¡Œå®Œæ•´æµ‹è¯•
python3 run_hybrid_fixed.py --data_file extended_dataset_50.json
```

### é€‰é¡¹2ï¼šå¿«é€ŸéªŒè¯ï¼ˆ5åˆ†é’Ÿï¼‰

```bash
# åªæµ‹è¯•LLMæ˜¯å¦æ­£å¸¸
python3 -c "
from hybrid_generator import HybridGenerator
from rule_matcher import RuleMatcher

matcher = RuleMatcher([], confidence_threshold=0.9)
gen = HybridGenerator(matcher, device='cuda')

code = 'df.append(row)'
new_code, strategy, conf = gen.generate(code, 'Update append to concat')
print(f'ç”Ÿæˆ: {new_code}')
print(f'ç­–ç•¥: {strategy}')
"
```

---

## âœ… æˆåŠŸæ ‡å‡†

ä¿®å¤æˆåŠŸçš„åˆ¤æ–­æ ‡å‡†ï¼š

- [ ] è®­ç»ƒé›† â‰¥ 30ä¸ªæ ·æœ¬
- [ ] LLMä¸ç”Ÿæˆæ„Ÿå¹å·
- [ ] ç²¾ç¡®åŒ¹é…ç‡ â‰¥ 75%
- [ ] è§„åˆ™è¦†ç›–ç‡ â‰¥ 50%

---

**ç«‹å³ä¸Šä¼ ä¿®å¤æ–‡ä»¶å¹¶é‡æ–°è¿è¡Œï¼** ğŸš€
