# ä½¿ç”¨å…¬å¼€æ•°æ®é›†è¿è¡Œæ··åˆç³»ç»Ÿ

## ğŸŒ å¿«é€Ÿå¼€å§‹

### æ­¥éª¤1ï¼šç”Ÿæˆå…¬å¼€æ•°æ®é›†

```bash
cd ~/api_migration_exp/scripts

# ç”ŸæˆåŸºäºçœŸå®è¿ç§»æ¨¡å¼çš„å…¬å¼€æ•°æ®é›†
python3 fetch_public_dataset.py
```

**ç”Ÿæˆçš„æ•°æ®é›†**ï¼š
- **æ–‡ä»¶å**: `public_dataset.json`
- **è®­ç»ƒé›†**: ~50æ ·æœ¬ï¼ˆ75%ï¼‰
- **æµ‹è¯•é›†**: ~15æ ·æœ¬ï¼ˆ25%ï¼‰
- **æ•°æ®æ¥æº**: TensorFlow/Pandas/Scikit-learn/NumPy/PyTorchå®˜æ–¹æ–‡æ¡£
- **è´¨é‡**: â­â­â­â­â­ï¼ˆçœŸå®å®˜æ–¹è¿ç§»æ¡ˆä¾‹ï¼‰

---

### æ­¥éª¤2ï¼šè¿è¡Œæ··åˆç³»ç»Ÿ

```bash
# ä½¿ç”¨å…¬å¼€æ•°æ®é›†è¿è¡Œ
python3 run_hybrid_system_fixed.py public_dataset.json
```

---

## ğŸ“Š æ•°æ®é›†è¯¦æƒ…

### åŒ…å«çš„è¿ç§»æ¨¡å¼

#### 1. **TensorFlow (14ä¸ªæ ·æœ¬)**
- `tf.contrib.layers.flatten(x)` â†’ `tf.keras.layers.Flatten()(x)`
- `tf.Session()` â†’ `# TF 2.xé»˜è®¤eageræ¨¡å¼`
- `tf.placeholder()` â†’ `tf.keras.Input()`
- `tf.train.AdamOptimizer()` â†’ `tf.keras.optimizers.Adam()`

#### 2. **Pandas (16ä¸ªæ ·æœ¬)**
- `df.append(row)` â†’ `pd.concat([df, row])`
- `df.ix[0]` â†’ `df.loc[0]`
- `df.sort('col')` â†’ `df.sort_values('col')`
- `pd.rolling_mean()` â†’ `data.rolling().mean()`

#### 3. **Scikit-learn (7ä¸ªæ ·æœ¬)**
- `from sklearn.cross_validation` â†’ `from sklearn.model_selection`
- `scaler.fit_transform(X)` â†’ `scaler.fit(X).transform(X)`

#### 4. **NumPy (7ä¸ªæ ·æœ¬)**
- `np.matrix()` â†’ `np.array()`
- `arr.tostring()` â†’ `arr.tobytes()`
- `np.rank()` â†’ `np.ndim()`

#### 5. **PyTorch (9ä¸ªæ ·æœ¬)**
- `torch.save(model)` â†’ `torch.save(model.state_dict())`
- `model.cuda()` â†’ `model.to('cuda')`
- `Variable(tensor)` â†’ `tensor`

---

## ğŸ” æ•°æ®é›†è´¨é‡å¯¹æ¯”

| æ•°æ®é›† | æ ·æœ¬æ•° | çœŸå®æ€§ | å¤šæ ·æ€§ | æ¨èåº¦ |
|--------|--------|--------|--------|--------|
| mini_dataset.json | 40 | â­â­â­ | â­â­ | âŒ å¤ªå° |
| large_dataset.json | 130 | â­â­â­â­ | â­â­â­ | âœ… å¥½ |
| public_dataset.json | 60+ | â­â­â­â­â­ | â­â­â­â­â­ | âœ…âœ… æœ€ä½³ |

---

## ğŸ“ˆ é¢„æœŸæ€§èƒ½

ä½¿ç”¨å…¬å¼€æ•°æ®é›†ï¼Œé¢„æœŸæŒ‡æ ‡ï¼š

```
ç²¾ç¡®åŒ¹é…ç‡: 50-70%
å¹³å‡ç›¸ä¼¼åº¦: 0.65-0.80
å…³é”®APIå‡†ç¡®ç‡: 70-85%
è§„åˆ™ç›´æ¥åº”ç”¨å‡†ç¡®ç‡: 85-95%
```

---

## ğŸš€ è¿›é˜¶ï¼šæ‰©å±•æ•°æ®é›†

### æ–¹æ³•1ï¼šå¢åŠ æ›´å¤šåº“

ç¼–è¾‘ `fetch_public_dataset.py`ï¼Œæ·»åŠ ï¼š
- Matplotlibè¿ç§»
- Requestsè¿ç§»
- PIL/Pillowè¿ç§»
- Djangoè¿ç§»

### æ–¹æ³•2ï¼šä»GitHubè·å–çœŸå®é¡¹ç›®æ•°æ®

```bash
# å…‹éš†åŒ…å«çœŸå®è¿ç§»çš„é¡¹ç›®
git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
git log --all --grep="migrate" --oneline

# æå–è¿ç§»commitçš„diff
git show <commit_hash>
```

### æ–¹æ³•3ï¼šä½¿ç”¨APIMigrationBench

```bash
# ä¸‹è½½APIMigrationBenchï¼ˆå¦‚æœå¯ç”¨ï¼‰
git clone https://github.com/NAIST-SE/APIMigrationBench.git
cd APIMigrationBench

# è½¬æ¢ä¸ºæˆ‘ä»¬çš„æ ¼å¼
python3 convert_api_bench.py
```

---

## ğŸ› ï¸ æ•…éšœæ’é™¤

### é—®é¢˜1ï¼šæ•°æ®é›†å¤ªå°
**è§£å†³**: è¿è¡Œå¤šæ¬¡`fetch_public_dataset.py`ç”Ÿæˆæ›´å¤šå˜ä½“

### é—®é¢˜2ï¼šæŸäº›åº“çš„è¿ç§»æ ·æœ¬ä¸è¶³
**è§£å†³**: æ‰‹åŠ¨æ·»åŠ åˆ°`fetch_public_dataset.py`ç›¸åº”æ–¹æ³•ä¸­

### é—®é¢˜3ï¼šè§„åˆ™å­¦ä¹ æ•ˆæœä¸å¥½
**è§£å†³**: 
- å¢åŠ è®­ç»ƒæ ·æœ¬æ•°é‡ï¼ˆ75% â†’ 80%ï¼‰
- æ£€æŸ¥æ•°æ®è´¨é‡ï¼ˆç¡®ä¿old_codeå’Œnew_codeçœŸå®å¯¹åº”ï¼‰

---

## ğŸ“ æ•°æ®é›†æ ¼å¼

```json
{
  "train": [
    {
      "id": "tf_1",
      "old_code": "tf.contrib.layers.flatten(x)",
      "new_code": "tf.keras.layers.Flatten()(x)",
      "dependency": "tensorflow",
      "description": "contrib.layers.flattenå·²ç§»é™¤",
      "source": "TensorFlow Official Guide"
    }
  ],
  "test": [...]
}
```

---

## âœ… éªŒè¯æ•°æ®é›†è´¨é‡

```bash
# æŸ¥çœ‹æ•°æ®é›†ç»Ÿè®¡
python3 -c "
import json
with open('public_dataset.json') as f:
    data = json.load(f)
    print(f'è®­ç»ƒé›†: {len(data[\"train\"])}')
    print(f'æµ‹è¯•é›†: {len(data[\"test\"])}')
    
    # ç»Ÿè®¡æ¯ä¸ªåº“çš„æ ·æœ¬æ•°
    libs = {}
    for sample in data['train']:
        lib = sample['dependency']
        libs[lib] = libs.get(lib, 0) + 1
    
    for lib, count in sorted(libs.items()):
        print(f'{lib}: {count}')
"
```
