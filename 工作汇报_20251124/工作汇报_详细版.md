# API版本迁移实验 - 详细工作汇报

**汇报人**：张昌宇  
**日期**：2025-11-24  
**项目周期**：2025-11-17 至今（约7天）

---

## 一、项目背景与目标

### 1.1 研究问题
如何让代码生成模型快速适应API版本更新，避免频繁全量微调和灾难性遗忘？

### 1.2 研究方案
探索三个并行方向：
1. **方向1**：强化学习/偏好微调（LoRA层次化设计）
2. **方向2**：神经知识编辑（ROME/MEMIT）
3. **方向3**：规则+Prompt工程（自动规则学习+混合生成）

### 1.3 实验环境
- **硬件**：服务器RTX 3090 24GB
- **软件**：Python 3.10, PyTorch 2.6+cu118, Transformers 4.44
- **模型**：Qwen/Qwen2.5-Coder-1.5B (1.5B参数)

---

## 二、整体进度总览

### 2.1 时间线
```
2025-11-17: 环境配置、框架搭建、Baseline实验
2025-11-19: LoRA方案实施（失败）
2025-11-20: ROME知识编辑尝试（失败）
2025-11-21: 数据集扩展、规则引导修复
2025-11-24: 持续优化中（当前）
```

### 2.2 完成度
| 阶段 | 任务 | 状态 | 完成度 |
|------|------|------|--------|
| 阶段1 | 环境配置 | ✅ 完成 | 100% |
| 阶段2 | 框架搭建 | ✅ 完成 | 100% |
| 阶段3 | 方向3实现 | 🚀 进行中 | 85% |
| 阶段3 | 方向1实现 | ⏸️ 搁置 | 50% |
| 阶段3 | 方向2实现 | ❌ 失败 | 20% |

---

## 三、各方向详细进展

### 3.1 方向3：规则+Prompt混合系统 ⭐ **重点**

#### 3.1.1 Baseline实验（2025-11-17）
**目标**：建立性能基线

**实施内容**：
- 实现4种Prompt策略：basic、with_context、with_rules、CoT
- 在10个测试样本上评估

**实验结果**：
| 策略 | 精确匹配 | 平均相似度 | 关键API准确率 |
|------|----------|------------|---------------|
| **basic** | **90.0%** | **0.98** | **90.0%** |
| with_context | 80.0% | 0.98 | 90.0% |
| with_rules | 70.0% | 0.87 | 90.0% |
| CoT | 70.0% | 0.75 | 90.0% |

**结论**：简单直接的Prompt效果最好

---

#### 3.1.2 混合系统深化（2025-11-21至今）

**a) 自动规则学习器** ✅
- **功能**：从训练数据自动提取API迁移规则
- **技术**：AST解析 + Regex匹配
- **成果**：从200个训练样本学到139条规则
- **规则类型**：
  - API替换（63条）：如 `df.append()` → `pd.concat()`
  - 参数迁移（11条）：参数增删
  - 语法模式（65条）：代码结构变化

**b) 智能规则匹配器** ✅
- **功能**：匹配旧代码与规则库
- **技术**：模糊匹配 + 置信度评分
- **特色**：特殊处理复杂迁移（如append→concat的参数映射）

**c) 混合生成器** ✅
- **三层策略**：
  1. 高置信度(≥0.85) → 规则直接应用
  2. 中置信度(0.6-0.85) → 规则引导LLM
  3. 低置信度(<0.6) → 纯LLM兜底
- **当前问题**：规则引导层生成空白（正在修复）

**d) 大规模数据集** ✅
- **来源**：TensorFlow、Pandas、scikit-learn、NumPy、PyTorch官方文档
- **规模**：
  - 训练集：240+样本
  - 测试集：60+样本
  - 覆盖：5个主流库，60+种迁移模式

---

#### 3.1.3 当前性能（2025-11-24）

**测试配置**：
- 数据集：public_dataset.json
- 测试样本：51个
- 学习规则：139条

**性能指标**：
| 指标 | 数值 | 评价 |
|------|------|------|
| 精确匹配率 | 23.5% | ⚠️ 低于预期（目标60%+） |
| 平均相似度 | 0.806 | ✅ 较好 |
| 关键API准确率 | 58.8% | ✅ 中等 |

**策略分布**：
| 策略 | 使用次数 | 准确率 | 评价 |
|------|----------|--------|------|
| 规则直接应用 | 21 (41.2%) | 57.1% | ✅ 可靠 |
| 规则引导Prompt | 29 (56.9%) | 0.0% | ❌ 待修复 |
| LLM兜底 | 1 (2.0%) | 0.0% | - |

**失败案例示例**：
```python
# 案例1
旧代码: input.cuda()
期望: input.to('cuda')
生成: input.cuda()  # ← 返回原代码
策略: rule_guided

# 案例2
旧代码: table.sort(['col1', 'col2'])
期望: table.sort_values(['col1', 'col2'])
生成: table.sort(['col1', 'col2'])  # ← 返回原代码
策略: rule_guided
```

---

### 3.2 方向1：LoRA微调 ⏸️ **暂时搁置**

#### 3.2.1 目标与设计
- **目标**：通过层次化LoRA减少参数量，避免灾难性遗忘
- **创新点**：只更新深层（22-31层），参数量减少69%
- **假设**：深层负责语义理解，更新深层足以适配API变化

#### 3.2.2 实施过程
**实验1**：Mini数据集快速测试（2025-11-17）
- 数据：3训练 + 10测试
- 结果：✅ 流程验证成功（精确匹配0%但数据太少）

**实验2-5**：50样例数据集（2025-11-19）
- 4次训练实验（层次化LoRA v1-v4 + 标准LoRA）
- 结果：❌ **全部失败，loss=NaN**

#### 3.2.3 问题诊断
**根本原因**：PEFT库与Qwen2.5-Coder的兼容性问题

**深度诊断过程**：
1. 怀疑transformers版本太新 → 降级 4.57→4.44（无效）
2. 怀疑训练参数问题 → 调整学习率、batch size（无效）
3. 怀疑数据处理问题 → 修复labels、DataCollator（无效）
4. 怀疑梯度问题 → 添加梯度裁剪（无效）
5. 确认PEFT兼容性问题 → 测试其他模型、降级PEFT（依然失败）

**时间投入**：约1.5天

#### 3.2.4 决策
- ⏸️ 暂时搁置LoRA方案
- 🎯 转向DPO（Direct Preference Optimization）作为替代
- 📅 待方向3稳定后再回归

---

### 3.3 方向2：ROME知识编辑 ❌ **已放弃**

#### 3.3.1 目标
直接修改模型内部知识，精确编辑API相关参数

#### 3.3.2 实施尝试
**尝试1**：简化版ROME实现
- 问题：维度不匹配错误
- 结果：0/10编辑成功

**尝试2**：EasyEdit库集成
- 问题：依赖冲突（与现有环境不兼容）
- 结果：安装失败

**尝试3**：直接ROME实现
- 问题：模型结构理解不足，维度匹配困难
- 结果：0/10编辑成功

**时间投入**：约0.5天

#### 3.3.3 决策
- ❌ 彻底放弃此方向
- 📝 论文中说明尝试失败原因
- 💡 知识编辑在代码模型上的应用可能需要更深入研究

---

## 四、遇到的困难与解决方案

### 4.1 困难1：LoRA训练数值崩溃 ⚠️ **未解决**

**现象**：
```
训练开始后立即：
Step 1: loss = nan
Step 2: loss = nan
...
```

**已尝试的解决方案**：
1. ✅ 降级transformers（4.57→4.44）
2. ✅ 调整学习率（1e-4, 5e-5, 1e-5）
3. ✅ 修复数据标签（正确设置labels）
4. ✅ 添加梯度裁剪（max_norm=1.0）
5. ✅ 使用DataCollatorForSeq2Seq
6. ✅ 验证数据质量
7. ❌ 降级PEFT（尝试中）

**根本原因**（推测）：
- PEFT库与Qwen2.5-Coder存在深层不兼容
- 可能需要切换到其他模型（如CodeLlama）

**当前状态**：暂时搁置

---

### 4.2 困难2：ROME维度匹配失败 ❌ **已放弃**

**现象**：
```
RuntimeError: The size of tensor a (4096) must match 
the size of tensor b (5120) at non-singleton dimension 1
```

**问题根源**：
- Qwen模型结构与ROME算法假设不匹配
- 需要深入理解模型内部表示

**解决尝试**：
- 阅读ROME论文和代码
- 尝试调整维度映射
- 寻找适配代码模型的版本

**结果**：无法在短时间内解决，投入产出比低

**决策**：放弃此方向

---

### 4.3 困难3：规则引导生成空白 ⚙️ **正在解决**

**现象**：
- 规则引导策略使用29次
- 29次全部返回原代码
- 准确率0%

**原因分析**：
1. **采样参数问题**：
   - `temperature=0.3` 太低，导致概率分布出现inf/nan
   - 错误：`RuntimeError: probability tensor contains either inf, nan`

2. **输出过滤过严**：
   - 多样性阈值30%太高
   - 误杀正常代码

3. **Prompt设计问题**：
   - 太复杂，模型难以理解

**解决方案**（2025-11-24实施）：
1. ✅ 改用greedy解码（`temperature=0.0`触发）
2. ✅ 放宽多样性阈值（30%→20%→10%）
3. ✅ 简化prompt：
   ```
   旧："{old_code}\nReplace {old_api} with {new_api}:"
   新："Update: {old_code}\nChange '{old_api}' to '{new_api}'\nResult:"
   ```
4. ✅ 提高采样最低温度（0.1→0.3）

**当前状态**：代码已修复，等待上传服务器验证

**预期改进**：
- 规则引导准确率：0% → 30-50%
- 精确匹配率：23.5% → 35-50%

---

## 五、核心创新点总结

### 5.1 自动规则学习
**传统方法**：手工编写规则
**本研究**：
- 从训练数据自动提取
- 支持参数映射识别
- 识别6种转换类型

**优势**：
- 可扩展性强
- 减少人工干预
- 适应新API模式

---

### 5.2 三层混合策略
**创新点**：动态选择生成策略

**分层设计**：
```
Layer 1 (高置信) → 快速准确的规则应用
Layer 2 (中置信) → 规则引导的智能生成
Layer 3 (低置信) → LLM兜底保证覆盖
```

**优势**：
- 平衡准确率和覆盖率
- 自适应选择最优策略
- 可解释性强

---

### 5.3 大规模真实数据集
**创新点**：基于官方文档的真实迁移案例

**数据来源**：
- TensorFlow迁移指南
- Pandas更新日志
- scikit-learn API变更文档
- NumPy版本说明
- PyTorch迁移手册

**质量保证**：
- 官方认证的迁移模式
- 真实代码场景
- 覆盖多样化API变化

---

## 六、性能分析

### 6.1 方向3性能总结

**优势**：
- ✅ 规则直接应用准确率57.1%（可靠）
- ✅ 平均相似度0.806（较高）
- ✅ 关键API准确率58.8%（中等）
- ✅ 无需训练，推理快速

**劣势**：
- ❌ 精确匹配率23.5%（低于预期）
- ❌ 规则引导策略失效（正在修复）
- ⚠️ 依赖规则质量

---

### 6.2 与Baseline对比

| 指标 | Baseline (10样本) | 混合系统 (51样本) |
|------|-------------------|-------------------|
| 精确匹配率 | 90.0% | 23.5% ⚠️ |
| 平均相似度 | 0.98 | 0.806 |
| 关键API准确率 | 90.0% | 58.8% |

**差距原因**：
1. 测试数据规模扩大（10→51）
2. 数据难度提升（真实复杂场景）
3. 规则引导策略待修复

**合理预期**：
- 精确匹配率：45-60%
- 平均相似度：0.80-0.85
- 关键API准确率：65-75%

---

## 七、资源消耗统计

### 7.1 计算资源
| 资源类型 | 用量 | 用途 |
|----------|------|------|
| GPU时间 | ~20小时 | 模型推理、训练尝试 |
| GPU显存 | 24GB | RTX 3090 |
| CPU | 32核 | 数据处理 |
| 内存 | 64GB | 大规模数据加载 |

### 7.2 存储资源
| 类型 | 大小 | 说明 |
|------|------|------|
| 模型缓存 | ~8GB | Qwen2.5-Coder |
| 数据集 | ~5MB | JSON格式 |
| 结果文件 | ~50MB | 推理输出 |
| 代码仓库 | ~10MB | Python脚本 |

### 7.3 时间投入
| 任务 | 时间 | 占比 |
|------|------|------|
| 方向3开发 | 2.5天 | 50% |
| LoRA调试 | 1.5天 | 30% |
| 数据集扩展 | 0.5天 | 10% |
| ROME尝试 | 0.5天 | 10% |
| **总计** | **5天** | **100%** |

---

## 八、下一步计划

### 8.1 近期（本周，2025-11-24至11-30）

**优先级1：验证规则引导修复** 🔴
- [ ] 上传修复后的代码
- [ ] 运行完整测试
- [ ] 分析改进效果
- [ ] 迭代优化

**优先级2：提升精确匹配率** 🟡
- [ ] 改进规则学习算法
- [ ] 优化规则匹配策略
- [ ] 调整置信度阈值
- [ ] 目标：45-50%精确匹配率

**优先级3：数据集扩展** 🟢
- [ ] 添加更多库的API迁移案例
- [ ] 扩展到500+样本
- [ ] 平衡不同库的分布

---

### 8.2 中期（1-2周，12月初）

**目标1：实现DPO方案**
- [ ] 编写DPO训练脚本
- [ ] 构造偏好数据集（chosen/rejected pairs）
- [ ] 训练并评估
- [ ] 与方向3对比

**目标2：完善实验对比**
- [ ] 统一评估框架
- [ ] 在同一测试集上评估所有方法
- [ ] 制作对比表格和图表
- [ ] 消融实验

**目标3：失败案例深度分析**
- [ ] 分类失败原因
- [ ] 统计失败模式
- [ ] 提出改进方向

---

### 8.3 远期（时间待定）

**取决于以下因素**：
1. 方向3最终性能
2. DPO方案是否成功
3. 学长的指导建议
4. 论文撰写时间表

**可能的任务**：
- 方向1（LoRA）重新尝试
- 探索其他微调方法
- 准备论文撰写
- 实验完善和补充

---

## 九、需要讨论的问题

### 9.1 方向选择

**问题1**：是否继续尝试LoRA方案？
- **选项A**：放弃LoRA，全力DPO
  - 优势：避免卡在技术问题上
  - 劣势：失去层次化LoRA的创新点

- **选项B**：尝试切换模型（如CodeLlama）
  - 优势：可能解决兼容性问题
  - 劣势：需要重新配置环境

- **选项C**：暂时搁置，等方向3完成后再回归
  - 优势：优先保证一个方向完整
  - 劣势：可能时间不够

**问题2**：方向2（ROME）是否彻底放弃？
- 已投入0.5天，3次尝试失败
- 技术难度远超预期
- 建议：论文中简要说明尝试即可

---

### 9.2 性能预期

**问题3**：当前23.5%精确匹配率，合理目标是多少？

**参考数据**：
- Baseline（简单数据）：90%
- 当前（复杂数据）：23.5%
- 修复后预期：35-50%

**需要确认**：
- 50%精确匹配率是否足够作为论文贡献？
- 是否需要60%+才有说服力？
- 平均相似度0.80+是否可以作为补充指标？

---

### 9.3 研究贡献

**问题4**：方向3作为主要贡献是否可行？

**当前优势**：
- ✅ 自动规则学习（创新）
- ✅ 三层混合策略（新颖）
- ✅ 大规模真实数据集（实用）
- ✅ 无需训练（高效）

**潜在问题**：
- ❓ 性能是否足够亮眼？
- ❓ 是否需要与微调方法对比？
- ❓ 创新点是否足够强？

---

### 9.4 时间规划

**问题5**：实验何时可以停止？

**当前状态**：
- 方向3：85%完成度
- 方向1：50%完成度（搁置）
- 方向2：20%完成度（放弃）

**时间估算**：
- 方向3优化：1周
- DPO实现：1-2周
- 对比分析：1周
- **总计**：3-4周实验时间

**需要确认**：
- 何时开始准备论文？
- 论文准备需要多久？
- 投稿截止日期？

---

## 十、总结与展望

### 10.1 已取得的成果

**框架层面** ✅
1. 完整的实验框架（数据、模型、评估）
2. 可扩展的规则学习系统
3. 灵活的混合生成策略
4. 大规模真实数据集

**技术层面** ✅
1. 自动规则学习算法
2. 三层混合生成策略
3. 智能规则匹配器
4. 增强评估框架

**实验层面** ✅
1. Baseline性能验证（90%精确匹配）
2. 大规模测试（51样本）
3. 多方向探索（3个方向）
4. 详细失败分析

---

### 10.2 当前挑战

**技术挑战** ⚙️
1. 规则引导策略待修复（正在进行）
2. 精确匹配率需要提升（23.5%→45%+）
3. LoRA方案受阻（PEFT兼容性问题）

**研究挑战** 🤔
1. 性能目标不明确
2. 方向选择需要决策
3. 时间规划待确定

---

### 10.3 需要的支持

**技术支持** 🛠️
- LoRA兼容性问题的解决思路
- DPO实现的参考资料
- 性能优化的建议

**研究支持** 📚
- 方向选择的建议
- 性能预期的确认
- 创新点评估

**时间支持** ⏰
- 论文撰写时间表
- 实验优化停止点
- 阶段性里程碑

---

## 附录

### A. 文件清单

**核心脚本**：
1. `rule_learner.py` - 规则学习器
2. `rule_matcher.py` - 规则匹配器
3. `hybrid_generator.py` - 混合生成器
4. `evaluate_hybrid.py` - 评估器
5. `run_hybrid_system_fixed.py` - 主运行脚本
6. `fetch_public_dataset.py` - 数据集生成器
7. `test_rule_guided.py` - 测试脚本

**数据文件**：
1. `mini_dataset.json` - 小数据集（40样本）
2. `large_dataset.json` - 扩展数据集（100样本）
3. `public_dataset.json` - 公开数据集（300+样本）

**配置文件**：
1. `configs/rules.json` - 手工规则（3条）
2. `configs/learned_rules.json` - 自动规则（139条）

**文档**：
1. `README.md` - 项目说明
2. `docs/方向3快速开始.md` - 快速指南
3. `docs/工作日志-20251121.md` - 工作日志
4. `更新说明-20251121.md` - 更新说明

---

### B. 实验日志索引

**详细日志文件**：
- `docs/实验记录.md` - 完整实验记录
- `docs/工作日志-20251121.md` - 11月21日工作记录
- `results/hybrid/hybrid_summary_*.txt` - 各次运行摘要

---

### C. 参考资料

**论文**：
1. ReCode: Updating Code API Knowledge with RL
2. CodeUpdateArena: Benchmarking Knowledge Editing
3. LoRA: Low-Rank Adaptation of Large Language Models
4. ROME: Rank-One Model Editing

**文档**：
1. TensorFlow Migration Guide
2. Pandas API Changes
3. PyTorch Migration Tutorials

---

**汇报结束，期待学长的指导建议！** 🙏
